{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "-zTLHrFCT6KY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math as Math\n",
    "# Aim is to predict the marks of students of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "p0KHq8ZgTpU4",
    "outputId": "c45573ab-de02-4bb4-ed53-fdc1b3027e01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['no' 'M' 2 ... 6 18 118]\n",
      " ['yes' 'M' 2 ... 7 19 107]\n",
      " ['yes' 'M' 3 ... 5 18 108]\n",
      " ...\n",
      " ['yes' 'M' 2 ... 5 17 123]\n",
      " ['yes' 'F' 1 ... 8 17 104]\n",
      " ['yes' 'M' 1 ... 6 18 128]]\n",
      "[[-1.0 1.0 0.5 ... 0.7071067811865476 0.973328526784575\n",
      "  0.9564144928693534]\n",
      " [1.0 1.0 0.5 ... 0.7071067811865476 1.0 0.9107454992153875]\n",
      " [1.0 1.0 0.75 ... 0.8660254037844386 0.973328526784575 0.914991421995628]\n",
      " ...\n",
      " [1.0 1.0 0.5 ... 0.7071067811865476 0.9459053029269172\n",
      "  0.9764672918705589]\n",
      " [1.0 -1.0 0.25 ... 0.5 0.9459053029269172 0.8978872704229617]\n",
      " [1.0 1.0 0.25 ... 0.5 0.973328526784575 0.9961164901835046]]\n"
     ]
    }
   ],
   "source": [
    "# Use the file namd 'training data' to train the model\n",
    "'''No affect:-absences\n",
    "x^4 for freetime and study lol\n",
    "x^2 for study and age and travel(ln(x) better tbh)\n",
    "internet,IQ and gender - x^1\n",
    "sqrt:-travel and IQ\n",
    "Mult:\n",
    "'''\n",
    "data = pd.read_excel('Training data.xlsx')\n",
    "x_train = np.array(data.iloc[:,0:8])\n",
    "y_train = np.array(data.iloc[:,8]).reshape(-1,1)\n",
    "print(x_train)\n",
    "\n",
    "# Try plotting y_train with different features\n",
    "# To get an idea whether to add some features or not\n",
    "# Add some features if required in x_train\n",
    "\n",
    "# Also do label encoding for features not represented in numbers\n",
    "# refer the link if not know : https://youtu.be/589nCGeWG1w?si=t2Wa7LgbUOO4RooM\n",
    "\n",
    "def feature_changing(x_train):\n",
    "  # ---------\n",
    "    # Your code here\n",
    "  # ---------\n",
    "    for i in range(x_train.shape[0]):\n",
    "        if(x_train[i,0]=='yes'):\n",
    "            x_train[i,0]=1\n",
    "        else:\n",
    "            x_train[i,0]=-1\n",
    "    for i in range(x_train.shape[0]):\n",
    "        if(x_train[i,1]=='M'):\n",
    "            x_train[i,1]=1\n",
    "        else:\n",
    "            x_train[i,1]=-1\n",
    "    x=x_train\n",
    "    for i in range(x_train.shape[1]):\n",
    "        for j in range(i,x_train.shape[1]):\n",
    "            x=np.concatenate((x,x_train[:,i:i+1]*x_train[:,j:j+1]),axis=1) \n",
    "    \n",
    "    \n",
    "    #x=np.concatenate((x,x[:,2:7]**2),axis=1)\n",
    "    x=np.concatenate((x,(x[:,3:7]**3)/(20**3)),axis=1)\n",
    "    x=np.concatenate((x,(x[:,2:3])**0.5),axis=1)\n",
    "    x=np.concatenate((x,(x[:,6:8])**0.5),axis=1)\n",
    "    \n",
    "    for i in range(x.shape[1]):\n",
    "        x[:,i]=x[:,i]/(np.max(x[:,i]))\n",
    "    return x\n",
    "\n",
    "x_train = feature_changing(x_train)\n",
    "print(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 51)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "tYshvtYlVour"
   },
   "outputs": [],
   "source": [
    "def z_score(x_train):\n",
    "\n",
    "  # ---------\n",
    "    # write the code for feature scaling here\n",
    "    # Your code here\n",
    "  # ---------\n",
    "    x_mean=np.mean(x_train)\n",
    "    x_std=np.mean(x_train)\n",
    "    x_train=(x_train-x_mean)/x_std\n",
    "    return x_train,x_std,x_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "id": "O5dOwbNbWJWa"
   },
   "outputs": [],
   "source": [
    "def cost(x_train,y_train,w,b):\n",
    "\n",
    "  # ---------\n",
    "    # Your code here\n",
    "    # Use mean square error as cost function\n",
    "    # return cost\n",
    "  # ---------\n",
    "    m=x_train.shape[0]\n",
    "    s=0.0\n",
    "    for i in range(m):\n",
    "        s+=(((np.dot(w,x_train[i])+b-y_train[i])**2))\n",
    "    s=s/2*m\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "hW8p2cTNU74W"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x_train,y_train,w,b):\n",
    "    a=0.01\n",
    "  # ---------\n",
    "    # Your code here\n",
    "    # Choose learning rate yourself\n",
    "  # ---------\n",
    "    m=len(y_train)\n",
    "    n=x_train.shape[1]\n",
    "    dw=np.zeros(n)\n",
    "    tw=0.0\n",
    "    for j in range(n):\n",
    "        for i in range(m):\n",
    "            tw=tw+((np.dot(w,x_train[i])+b-y_train[i])*x_train[i,j])\n",
    "        dw[j]=tw\n",
    "    dw=dw/m\n",
    "    tb=0.0\n",
    "    for i in range(m):\n",
    "            tb=tb+((np.dot(w,x_train[i])+b-y_train[i]))\n",
    "    db=tb/m\n",
    "    tw=w-a*dw\n",
    "    tb=b-a*db\n",
    "    w=tw\n",
    "    b=tb\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.563674607362476\n",
      "[[73.13]\n",
      " [67.78]\n",
      " [70.84]\n",
      " [66.68]\n",
      " [71.88]\n",
      " [73.84]\n",
      " [71.47]\n",
      " [74.48]\n",
      " [73.48]\n",
      " [91.44]\n",
      " [79.38]\n",
      " [62.51]\n",
      " [75.  ]\n",
      " [62.64]\n",
      " [53.95]\n",
      " [68.26]\n",
      " [67.65]\n",
      " [76.44]\n",
      " [77.44]\n",
      " [78.45]\n",
      " [77.23]\n",
      " [70.31]\n",
      " [80.  ]\n",
      " [69.9 ]\n",
      " [65.42]\n",
      " [67.38]\n",
      " [73.17]\n",
      " [81.11]\n",
      " [82.87]\n",
      " [72.4 ]\n",
      " [65.  ]\n",
      " [75.49]\n",
      " [73.74]\n",
      " [68.4 ]\n",
      " [64.06]\n",
      " [79.62]\n",
      " [60.55]\n",
      " [79.06]\n",
      " [61.99]\n",
      " [77.3 ]\n",
      " [81.22]\n",
      " [67.43]\n",
      " [72.16]\n",
      " [67.11]\n",
      " [67.78]\n",
      " [62.99]\n",
      " [65.14]\n",
      " [56.96]\n",
      " [73.75]\n",
      " [64.25]\n",
      " [71.61]\n",
      " [66.72]\n",
      " [61.73]\n",
      " [75.25]\n",
      " [80.37]\n",
      " [80.59]\n",
      " [67.13]\n",
      " [75.13]\n",
      " [78.27]\n",
      " [58.17]\n",
      " [76.88]\n",
      " [62.38]\n",
      " [63.09]\n",
      " [73.21]\n",
      " [72.03]\n",
      " [66.38]\n",
      " [78.91]\n",
      " [76.35]\n",
      " [65.27]\n",
      " [80.82]\n",
      " [60.83]\n",
      " [65.23]\n",
      " [74.39]\n",
      " [71.04]\n",
      " [68.27]\n",
      " [81.3 ]\n",
      " [72.11]\n",
      " [69.73]\n",
      " [63.11]\n",
      " [65.88]\n",
      " [61.36]\n",
      " [74.8 ]\n",
      " [72.86]\n",
      " [69.6 ]\n",
      " [67.93]\n",
      " [76.35]\n",
      " [66.52]\n",
      " [72.81]\n",
      " [74.36]\n",
      " [73.42]\n",
      " [78.12]\n",
      " [66.41]\n",
      " [66.98]\n",
      " [67.36]\n",
      " [65.7 ]\n",
      " [78.4 ]\n",
      " [76.09]\n",
      " [74.08]\n",
      " [55.33]\n",
      " [66.71]\n",
      " [58.59]\n",
      " [64.59]\n",
      " [59.55]\n",
      " [67.06]\n",
      " [67.72]\n",
      " [63.57]\n",
      " [60.73]\n",
      " [64.01]\n",
      " [77.23]\n",
      " [65.02]\n",
      " [70.86]\n",
      " [64.08]\n",
      " [86.8 ]\n",
      " [69.4 ]\n",
      " [55.62]\n",
      " [63.21]\n",
      " [63.09]\n",
      " [77.43]\n",
      " [73.31]\n",
      " [63.23]\n",
      " [57.34]\n",
      " [73.67]\n",
      " [70.12]\n",
      " [64.47]\n",
      " [75.16]\n",
      " [71.61]\n",
      " [70.48]\n",
      " [54.23]\n",
      " [67.95]\n",
      " [72.43]\n",
      " [68.03]\n",
      " [65.81]\n",
      " [66.22]\n",
      " [66.18]\n",
      " [70.04]\n",
      " [84.72]\n",
      " [75.88]\n",
      " [76.89]\n",
      " [81.34]\n",
      " [76.02]\n",
      " [62.17]\n",
      " [68.52]\n",
      " [67.89]\n",
      " [66.47]\n",
      " [73.74]\n",
      " [63.72]\n",
      " [64.08]\n",
      " [67.99]\n",
      " [72.33]\n",
      " [64.48]\n",
      " [59.44]\n",
      " [68.42]\n",
      " [67.42]\n",
      " [60.52]\n",
      " [75.15]\n",
      " [55.11]\n",
      " [71.51]\n",
      " [82.37]\n",
      " [70.96]\n",
      " [63.17]\n",
      " [68.92]\n",
      " [84.23]\n",
      " [71.12]\n",
      " [83.65]\n",
      " [67.71]\n",
      " [84.07]\n",
      " [67.42]\n",
      " [73.61]\n",
      " [67.25]\n",
      " [56.25]\n",
      " [62.26]\n",
      " [75.8 ]\n",
      " [75.18]\n",
      " [77.4 ]\n",
      " [68.88]\n",
      " [78.83]\n",
      " [67.62]\n",
      " [73.88]\n",
      " [65.16]\n",
      " [62.8 ]\n",
      " [56.98]\n",
      " [64.83]\n",
      " [75.52]\n",
      " [69.11]\n",
      " [73.5 ]\n",
      " [70.13]\n",
      " [66.12]\n",
      " [64.37]\n",
      " [73.55]\n",
      " [68.78]\n",
      " [61.35]\n",
      " [85.11]\n",
      " [85.75]\n",
      " [74.  ]\n",
      " [69.5 ]\n",
      " [62.85]\n",
      " [61.67]\n",
      " [63.07]\n",
      " [75.25]\n",
      " [76.14]\n",
      " [66.04]\n",
      " [75.36]\n",
      " [70.52]\n",
      " [67.23]\n",
      " [60.82]\n",
      " [57.82]\n",
      " [64.66]\n",
      " [76.03]\n",
      " [77.34]\n",
      " [74.87]\n",
      " [72.31]\n",
      " [72.63]\n",
      " [56.33]\n",
      " [83.93]\n",
      " [75.59]\n",
      " [78.64]\n",
      " [61.06]\n",
      " [72.56]\n",
      " [81.9 ]\n",
      " [64.04]\n",
      " [85.09]\n",
      " [78.75]\n",
      " [82.52]\n",
      " [60.01]\n",
      " [61.07]\n",
      " [72.81]\n",
      " [65.92]\n",
      " [81.15]\n",
      " [61.99]\n",
      " [69.03]\n",
      " [62.85]\n",
      " [56.43]\n",
      " [71.52]\n",
      " [73.19]\n",
      " [70.08]\n",
      " [67.36]\n",
      " [59.36]\n",
      " [74.15]\n",
      " [71.39]\n",
      " [74.77]\n",
      " [63.91]\n",
      " [75.27]\n",
      " [63.48]\n",
      " [61.12]\n",
      " [65.11]\n",
      " [73.69]\n",
      " [66.27]\n",
      " [72.4 ]\n",
      " [62.81]\n",
      " [62.36]\n",
      " [76.62]\n",
      " [72.06]\n",
      " [69.79]\n",
      " [70.59]\n",
      " [76.3 ]\n",
      " [63.05]\n",
      " [68.55]\n",
      " [57.7 ]\n",
      " [65.22]\n",
      " [66.06]\n",
      " [69.17]\n",
      " [85.22]\n",
      " [77.94]\n",
      " [74.76]\n",
      " [61.66]\n",
      " [78.8 ]\n",
      " [71.02]\n",
      " [73.28]\n",
      " [73.3 ]\n",
      " [59.16]\n",
      " [72.84]\n",
      " [71.39]\n",
      " [70.96]\n",
      " [72.72]\n",
      " [74.5 ]\n",
      " [60.27]\n",
      " [71.94]\n",
      " [62.38]\n",
      " [61.12]\n",
      " [70.83]\n",
      " [69.99]\n",
      " [82.06]\n",
      " [68.59]\n",
      " [62.48]\n",
      " [73.31]\n",
      " [70.03]\n",
      " [75.84]\n",
      " [74.75]\n",
      " [72.33]\n",
      " [71.86]\n",
      " [58.98]\n",
      " [66.15]\n",
      " [61.95]\n",
      " [67.85]\n",
      " [75.16]\n",
      " [82.39]\n",
      " [70.22]\n",
      " [81.12]\n",
      " [67.94]\n",
      " [68.75]\n",
      " [78.11]\n",
      " [67.87]\n",
      " [69.91]\n",
      " [58.97]\n",
      " [77.66]\n",
      " [73.31]\n",
      " [71.77]\n",
      " [82.97]\n",
      " [72.57]\n",
      " [70.01]\n",
      " [61.72]\n",
      " [66.05]\n",
      " [83.5 ]\n",
      " [55.05]\n",
      " [65.06]\n",
      " [61.56]\n",
      " [65.64]\n",
      " [65.3 ]\n",
      " [61.05]\n",
      " [55.16]\n",
      " [59.67]\n",
      " [69.89]\n",
      " [63.1 ]\n",
      " [70.18]\n",
      " [71.84]\n",
      " [58.73]\n",
      " [64.15]\n",
      " [77.69]\n",
      " [73.01]\n",
      " [70.64]\n",
      " [72.28]\n",
      " [75.94]\n",
      " [62.37]\n",
      " [60.96]\n",
      " [60.74]\n",
      " [64.15]\n",
      " [69.47]\n",
      " [82.33]\n",
      " [74.77]\n",
      " [73.62]\n",
      " [66.14]\n",
      " [74.15]\n",
      " [66.12]\n",
      " [65.94]\n",
      " [68.8 ]\n",
      " [75.66]\n",
      " [73.97]\n",
      " [74.5 ]\n",
      " [77.91]\n",
      " [62.85]\n",
      " [70.19]\n",
      " [80.94]\n",
      " [68.45]\n",
      " [67.97]\n",
      " [67.55]\n",
      " [72.  ]\n",
      " [66.65]\n",
      " [58.98]\n",
      " [64.2 ]\n",
      " [86.53]\n",
      " [79.41]\n",
      " [77.16]\n",
      " [71.3 ]\n",
      " [74.22]\n",
      " [59.04]\n",
      " [73.07]\n",
      " [73.48]\n",
      " [80.83]\n",
      " [74.63]\n",
      " [66.11]\n",
      " [77.93]\n",
      " [65.66]\n",
      " [77.64]\n",
      " [69.54]\n",
      " [80.69]\n",
      " [64.47]\n",
      " [78.  ]\n",
      " [64.06]\n",
      " [68.02]\n",
      " [67.16]\n",
      " [80.41]\n",
      " [66.81]\n",
      " [64.38]\n",
      " [83.75]\n",
      " [78.37]\n",
      " [75.05]\n",
      " [66.72]\n",
      " [68.46]\n",
      " [63.41]\n",
      " [86.61]\n",
      " [78.1 ]\n",
      " [57.33]\n",
      " [74.93]\n",
      " [69.47]\n",
      " [77.7 ]\n",
      " [73.87]\n",
      " [83.28]\n",
      " [76.53]\n",
      " [77.99]\n",
      " [67.37]\n",
      " [69.55]\n",
      " [70.78]\n",
      " [62.  ]\n",
      " [69.33]\n",
      " [76.19]\n",
      " [66.43]\n",
      " [61.88]\n",
      " [69.07]\n",
      " [75.71]\n",
      " [73.63]\n",
      " [70.24]\n",
      " [68.06]\n",
      " [69.71]\n",
      " [64.22]\n",
      " [73.91]\n",
      " [75.48]\n",
      " [75.59]\n",
      " [63.62]\n",
      " [77.54]\n",
      " [60.65]\n",
      " [57.59]\n",
      " [73.89]\n",
      " [78.27]\n",
      " [62.08]\n",
      " [69.62]\n",
      " [65.72]\n",
      " [65.77]\n",
      " [85.41]\n",
      " [58.25]\n",
      " [62.2 ]\n",
      " [65.88]\n",
      " [77.94]\n",
      " [67.83]\n",
      " [83.59]\n",
      " [68.91]\n",
      " [64.13]\n",
      " [70.34]\n",
      " [78.97]\n",
      " [72.19]\n",
      " [69.18]\n",
      " [81.53]\n",
      " [67.25]\n",
      " [63.54]\n",
      " [58.52]\n",
      " [61.4 ]\n",
      " [63.31]\n",
      " [67.51]\n",
      " [75.5 ]\n",
      " [71.24]\n",
      " [63.17]\n",
      " [72.61]\n",
      " [71.14]\n",
      " [58.8 ]\n",
      " [72.3 ]\n",
      " [72.06]\n",
      " [78.78]\n",
      " [66.09]\n",
      " [80.93]\n",
      " [69.72]\n",
      " [53.52]\n",
      " [71.91]\n",
      " [77.37]\n",
      " [67.69]\n",
      " [63.21]\n",
      " [68.37]\n",
      " [64.69]\n",
      " [68.91]\n",
      " [76.6 ]\n",
      " [65.49]\n",
      " [61.05]\n",
      " [70.46]\n",
      " [62.36]\n",
      " [79.81]\n",
      " [86.53]\n",
      " [72.55]\n",
      " [64.11]\n",
      " [69.4 ]\n",
      " [62.51]\n",
      " [71.97]\n",
      " [65.5 ]\n",
      " [68.51]\n",
      " [78.57]\n",
      " [65.65]\n",
      " [68.67]\n",
      " [81.94]\n",
      " [70.05]\n",
      " [71.2 ]\n",
      " [63.46]\n",
      " [64.02]\n",
      " [76.25]\n",
      " [67.17]\n",
      " [79.63]\n",
      " [77.11]\n",
      " [80.71]\n",
      " [72.69]\n",
      " [69.18]\n",
      " [65.75]\n",
      " [67.99]\n",
      " [75.43]\n",
      " [61.54]\n",
      " [64.7 ]\n",
      " [74.24]\n",
      " [59.25]\n",
      " [63.09]\n",
      " [79.89]\n",
      " [67.78]\n",
      " [64.69]\n",
      " [71.41]\n",
      " [60.56]\n",
      " [70.86]\n",
      " [63.45]\n",
      " [76.66]\n",
      " [69.63]\n",
      " [76.42]\n",
      " [68.35]\n",
      " [77.75]\n",
      " [66.69]\n",
      " [75.09]\n",
      " [83.28]\n",
      " [69.53]\n",
      " [69.19]\n",
      " [72.06]\n",
      " [61.31]\n",
      " [77.35]\n",
      " [73.09]\n",
      " [69.86]\n",
      " [62.93]\n",
      " [65.23]\n",
      " [65.05]\n",
      " [63.8 ]\n",
      " [68.28]\n",
      " [67.89]\n",
      " [70.05]\n",
      " [70.05]\n",
      " [64.46]\n",
      " [77.59]\n",
      " [66.28]\n",
      " [75.87]\n",
      " [71.24]\n",
      " [71.85]\n",
      " [81.41]\n",
      " [65.62]\n",
      " [64.38]\n",
      " [62.59]\n",
      " [82.5 ]\n",
      " [64.01]\n",
      " [64.  ]\n",
      " [69.7 ]\n",
      " [59.65]\n",
      " [70.34]\n",
      " [78.52]\n",
      " [65.09]\n",
      " [74.75]\n",
      " [78.95]\n",
      " [71.88]\n",
      " [69.76]\n",
      " [83.84]\n",
      " [72.  ]\n",
      " [58.09]\n",
      " [64.92]\n",
      " [73.12]\n",
      " [76.52]\n",
      " [67.51]\n",
      " [75.62]\n",
      " [70.54]\n",
      " [55.61]\n",
      " [61.71]\n",
      " [68.4 ]\n",
      " [67.55]\n",
      " [78.31]\n",
      " [68.23]\n",
      " [62.47]\n",
      " [68.6 ]\n",
      " [69.82]\n",
      " [72.61]\n",
      " [82.36]\n",
      " [59.63]\n",
      " [65.38]\n",
      " [86.67]\n",
      " [71.34]\n",
      " [65.77]\n",
      " [74.67]\n",
      " [59.91]\n",
      " [61.69]\n",
      " [56.25]\n",
      " [67.11]\n",
      " [55.22]\n",
      " [82.57]\n",
      " [70.09]\n",
      " [74.16]\n",
      " [80.2 ]\n",
      " [69.86]\n",
      " [63.05]\n",
      " [80.08]\n",
      " [78.86]\n",
      " [81.43]\n",
      " [69.21]\n",
      " [56.98]\n",
      " [61.98]\n",
      " [62.57]\n",
      " [67.31]\n",
      " [69.69]\n",
      " [68.71]\n",
      " [73.28]\n",
      " [75.06]\n",
      " [73.99]\n",
      " [80.81]\n",
      " [62.66]\n",
      " [63.11]\n",
      " [70.99]\n",
      " [65.03]\n",
      " [65.05]\n",
      " [69.36]\n",
      " [76.31]\n",
      " [68.06]\n",
      " [60.56]\n",
      " [69.43]\n",
      " [62.5 ]\n",
      " [67.97]\n",
      " [83.59]\n",
      " [72.16]\n",
      " [80.55]\n",
      " [75.39]\n",
      " [64.5 ]\n",
      " [65.89]\n",
      " [64.12]\n",
      " [76.86]\n",
      " [64.37]\n",
      " [77.11]\n",
      " [67.03]\n",
      " [73.16]\n",
      " [63.98]\n",
      " [67.36]\n",
      " [68.03]\n",
      " [77.3 ]\n",
      " [62.3 ]\n",
      " [73.78]\n",
      " [78.45]\n",
      " [81.45]\n",
      " [55.74]\n",
      " [64.81]\n",
      " [76.71]\n",
      " [76.03]\n",
      " [80.37]\n",
      " [73.26]\n",
      " [80.12]\n",
      " [68.9 ]\n",
      " [75.93]\n",
      " [73.64]\n",
      " [70.84]\n",
      " [71.59]\n",
      " [76.94]\n",
      " [71.14]\n",
      " [78.25]\n",
      " [68.11]\n",
      " [77.8 ]\n",
      " [77.49]\n",
      " [74.05]\n",
      " [80.  ]\n",
      " [78.93]\n",
      " [72.46]\n",
      " [82.28]\n",
      " [80.66]\n",
      " [75.5 ]\n",
      " [75.48]\n",
      " [64.5 ]\n",
      " [71.32]\n",
      " [75.61]\n",
      " [76.42]\n",
      " [74.72]\n",
      " [72.81]\n",
      " [70.26]\n",
      " [79.51]\n",
      " [72.14]\n",
      " [74.81]\n",
      " [60.24]\n",
      " [70.08]\n",
      " [78.43]\n",
      " [79.19]\n",
      " [76.68]\n",
      " [64.46]\n",
      " [80.75]\n",
      " [61.36]\n",
      " [78.5 ]\n",
      " [87.44]\n",
      " [64.49]\n",
      " [69.9 ]\n",
      " [72.03]\n",
      " [60.94]\n",
      " [60.65]\n",
      " [74.72]\n",
      " [75.41]\n",
      " [68.84]\n",
      " [70.07]\n",
      " [64.31]\n",
      " [77.22]\n",
      " [65.59]\n",
      " [72.5 ]\n",
      " [80.47]\n",
      " [75.98]\n",
      " [58.1 ]\n",
      " [75.76]\n",
      " [75.91]\n",
      " [72.67]\n",
      " [69.59]\n",
      " [56.42]\n",
      " [62.17]\n",
      " [80.18]\n",
      " [74.38]\n",
      " [65.23]\n",
      " [64.54]\n",
      " [67.31]\n",
      " [67.86]\n",
      " [61.24]\n",
      " [59.97]\n",
      " [70.28]\n",
      " [75.38]\n",
      " [56.25]\n",
      " [74.5 ]\n",
      " [75.92]\n",
      " [55.79]\n",
      " [67.93]\n",
      " [73.71]\n",
      " [60.58]\n",
      " [69.36]\n",
      " [72.1 ]\n",
      " [64.47]\n",
      " [65.04]\n",
      " [72.55]\n",
      " [78.07]\n",
      " [66.29]\n",
      " [63.68]\n",
      " [67.09]\n",
      " [63.82]\n",
      " [69.39]\n",
      " [71.58]\n",
      " [74.36]\n",
      " [61.19]\n",
      " [61.02]\n",
      " [67.74]\n",
      " [65.71]\n",
      " [74.05]\n",
      " [61.1 ]\n",
      " [70.08]\n",
      " [60.88]\n",
      " [66.09]\n",
      " [65.11]\n",
      " [59.58]\n",
      " [65.56]\n",
      " [79.91]\n",
      " [62.51]\n",
      " [68.78]\n",
      " [71.34]\n",
      " [64.75]\n",
      " [62.81]\n",
      " [59.04]\n",
      " [66.88]\n",
      " [59.98]\n",
      " [54.55]\n",
      " [66.06]\n",
      " [63.73]\n",
      " [72.68]\n",
      " [66.47]\n",
      " [67.71]\n",
      " [72.8 ]\n",
      " [64.72]\n",
      " [64.77]\n",
      " [77.18]\n",
      " [57.89]\n",
      " [78.11]\n",
      " [73.97]\n",
      " [68.45]\n",
      " [69.99]\n",
      " [65.63]\n",
      " [71.78]\n",
      " [75.68]\n",
      " [67.8 ]\n",
      " [75.98]\n",
      " [67.16]\n",
      " [71.74]\n",
      " [72.  ]\n",
      " [82.69]\n",
      " [71.67]\n",
      " [60.37]\n",
      " [72.11]\n",
      " [80.81]\n",
      " [60.32]\n",
      " [70.91]\n",
      " [62.09]\n",
      " [60.87]\n",
      " [65.49]\n",
      " [57.14]\n",
      " [79.9 ]\n",
      " [61.08]\n",
      " [87.34]\n",
      " [64.84]\n",
      " [74.93]\n",
      " [67.86]\n",
      " [72.74]\n",
      " [72.3 ]\n",
      " [75.06]\n",
      " [67.44]\n",
      " [77.83]\n",
      " [57.35]\n",
      " [63.19]\n",
      " [71.95]\n",
      " [74.82]\n",
      " [64.95]\n",
      " [80.31]\n",
      " [72.9 ]\n",
      " [70.21]\n",
      " [74.68]\n",
      " [65.16]\n",
      " [75.16]\n",
      " [60.51]\n",
      " [69.74]\n",
      " [68.16]\n",
      " [61.94]\n",
      " [66.76]\n",
      " [71.26]\n",
      " [71.66]\n",
      " [76.77]\n",
      " [71.34]\n",
      " [67.64]\n",
      " [69.17]\n",
      " [69.52]\n",
      " [67.7 ]\n",
      " [68.72]\n",
      " [64.65]\n",
      " [72.39]\n",
      " [64.59]\n",
      " [58.83]\n",
      " [70.66]\n",
      " [61.07]\n",
      " [69.75]\n",
      " [62.76]\n",
      " [69.93]\n",
      " [62.96]\n",
      " [75.16]\n",
      " [66.52]\n",
      " [64.9 ]\n",
      " [67.47]\n",
      " [55.87]\n",
      " [72.78]\n",
      " [83.22]\n",
      " [74.7 ]\n",
      " [82.49]\n",
      " [67.4 ]\n",
      " [72.74]\n",
      " [67.53]\n",
      " [70.26]\n",
      " [63.88]\n",
      " [70.5 ]\n",
      " [65.69]\n",
      " [82.03]\n",
      " [62.67]\n",
      " [70.1 ]\n",
      " [64.7 ]\n",
      " [77.12]\n",
      " [64.2 ]\n",
      " [64.31]\n",
      " [86.52]\n",
      " [87.91]\n",
      " [61.99]\n",
      " [69.98]\n",
      " [67.6 ]\n",
      " [62.15]\n",
      " [73.31]\n",
      " [54.66]\n",
      " [70.95]\n",
      " [69.08]\n",
      " [75.08]\n",
      " [62.19]\n",
      " [73.37]\n",
      " [73.44]\n",
      " [74.51]\n",
      " [73.89]\n",
      " [76.3 ]\n",
      " [56.61]\n",
      " [65.87]\n",
      " [65.35]\n",
      " [75.19]\n",
      " [71.71]\n",
      " [71.77]\n",
      " [68.33]\n",
      " [61.41]\n",
      " [62.74]\n",
      " [72.66]\n",
      " [69.14]\n",
      " [64.25]\n",
      " [72.42]\n",
      " [56.06]\n",
      " [75.47]\n",
      " [63.88]\n",
      " [77.71]\n",
      " [63.2 ]\n",
      " [70.62]\n",
      " [63.35]\n",
      " [69.24]\n",
      " [80.88]\n",
      " [63.25]\n",
      " [64.37]\n",
      " [71.9 ]\n",
      " [80.3 ]\n",
      " [65.  ]\n",
      " [64.44]\n",
      " [79.87]\n",
      " [67.73]\n",
      " [76.59]\n",
      " [79.28]\n",
      " [76.91]\n",
      " [66.52]\n",
      " [60.39]\n",
      " [57.96]\n",
      " [61.52]\n",
      " [70.83]\n",
      " [78.41]\n",
      " [54.97]\n",
      " [81.32]\n",
      " [70.36]\n",
      " [63.73]\n",
      " [75.84]\n",
      " [67.7 ]\n",
      " [74.87]\n",
      " [75.47]\n",
      " [71.6 ]\n",
      " [65.25]\n",
      " [69.4 ]\n",
      " [83.88]\n",
      " [75.19]\n",
      " [67.17]\n",
      " [68.87]\n",
      " [79.4 ]\n",
      " [76.22]\n",
      " [76.62]\n",
      " [79.12]\n",
      " [68.97]\n",
      " [68.71]\n",
      " [65.22]\n",
      " [59.09]\n",
      " [76.81]\n",
      " [76.71]\n",
      " [63.82]\n",
      " [73.18]\n",
      " [60.2 ]\n",
      " [67.3 ]\n",
      " [73.16]\n",
      " [59.91]\n",
      " [56.81]\n",
      " [80.73]\n",
      " [66.  ]\n",
      " [73.05]\n",
      " [58.59]\n",
      " [53.85]\n",
      " [73.25]\n",
      " [74.51]\n",
      " [70.38]\n",
      " [71.67]\n",
      " [66.68]\n",
      " [58.74]\n",
      " [71.38]\n",
      " [61.99]\n",
      " [71.26]\n",
      " [70.22]\n",
      " [69.23]\n",
      " [75.88]\n",
      " [79.18]\n",
      " [65.15]\n",
      " [67.61]\n",
      " [64.87]\n",
      " [72.3 ]\n",
      " [70.24]\n",
      " [80.59]\n",
      " [62.74]\n",
      " [79.08]\n",
      " [69.65]\n",
      " [71.18]\n",
      " [75.91]\n",
      " [56.77]\n",
      " [54.99]\n",
      " [62.95]\n",
      " [63.27]\n",
      " [70.02]\n",
      " [89.34]\n",
      " [74.26]\n",
      " [84.52]\n",
      " [73.56]\n",
      " [62.88]\n",
      " [80.27]\n",
      " [65.46]\n",
      " [77.68]\n",
      " [77.87]\n",
      " [68.77]\n",
      " [67.96]\n",
      " [62.12]\n",
      " [64.12]\n",
      " [75.47]\n",
      " [76.73]\n",
      " [69.07]\n",
      " [78.73]]\n"
     ]
    }
   ],
   "source": [
    "x_train,x_std,x_mean = z_score(x_train)\n",
    "print(np.max(x_train[:,:])-np.min(x_train[:,:]))\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "id": "Kl-fioJ5WkYn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adars\\AppData\\Local\\Temp\\ipykernel_6720\\2080162078.py:14: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  dw[j]=tw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost([ 1.36017654 -1.85386413  0.63518623  0.09649266 -1.43319635 -0.25411834\n",
      " -0.29781618  1.56905834  2.16144955  3.16953942  1.23666054  0.72916615\n",
      "  1.40980661 -1.3835993  -0.24055912 -0.23899904 -0.50357714 -0.60970382\n",
      " -1.43839457 -0.5123862  -3.34487    -1.59668828 -4.12926134 -2.64817086\n",
      " -4.43115637 -3.47939953 -3.64228174 -5.65115816 -6.3611572  -6.29549342\n",
      " -4.19593597 -4.39430559 -4.15384724 -4.64434804 -5.85814869 -4.19800662\n",
      " -5.67835637 -4.69304326 -4.6856169  -5.67505938 -4.19852003 -2.80523687\n",
      " -3.06892776 -3.30352386 -4.35821861 -2.81217912 -2.29528282 -3.32840084\n",
      " -2.35189342 -1.9393189  -1.46428985],[1.57820332],x_train,y_train)=[2.85244276e+09]\n",
      "cost([ 2.7474122   0.09474213  3.22608899  4.59190538  2.4945334   3.76449068\n",
      "  5.50635045  9.17966402 11.86449296  0.57463401 -1.55424569 -1.29092371\n",
      " -0.86599033 -2.87212004 -0.55487701  0.55640549  2.38426513  1.52930771\n",
      "  0.51618827  0.80880463 -1.59577233  0.54713363 -1.65409053 -0.20220459\n",
      " -1.69508226 -1.47702075 -2.135081   -3.66972336 -3.91331765 -2.41859101\n",
      " -0.35033242 -0.38255054  1.52065156  2.55003299 -0.36267337  0.08822667\n",
      " -2.08047175 -1.66395012 -2.36501855 -3.414957   -1.97808689  0.91892645\n",
      "  2.18603983  3.50618292  3.48445252  2.77477572  2.18514265  2.39672453\n",
      "  4.58801083  6.93630133  9.35612455],[3.2105772],x_train,y_train)=[9235986.88707933]\n",
      "cost([ 2.48618393 -0.14802927  3.02681883  3.96348655  1.59149794  3.50452794\n",
      "  5.10751685  8.92955299 11.55445568 -0.08952899 -1.91671455 -1.67336897\n",
      " -1.2491499  -2.13364743 -0.10363204  0.87317791  2.64111129  1.40629375\n",
      "  0.27062898  0.31358984 -1.23604675  0.89397744 -1.19984391  0.34463036\n",
      " -1.27576454 -1.07845744 -1.35025525 -2.87526374 -2.98139863 -2.03656902\n",
      " -0.33599483 -0.2351491   1.22086752  1.98016453 -1.31156509 -0.53401558\n",
      " -3.01133295 -2.69698051 -2.71245722 -3.19785176 -1.10909338  1.57857619\n",
      "  2.91661168  4.53139892  3.92488192  2.78276366  2.82851566  2.76785227\n",
      "  4.96040854  7.20818124  9.68155437],[3.16382692],x_train,y_train)=[4223218.93097936]\n",
      "cost([ 1.99761293 -0.46953981  2.51103636  3.50223339  1.3301669   3.22495903\n",
      "  4.83848429  8.8659921  11.60232756  0.06754454 -1.72324715 -1.46514662\n",
      " -1.10085567 -1.5479249  -0.0261069   0.58200708  2.46137323  0.94177813\n",
      " -0.12141454 -0.05171173 -1.01575324  1.23957931 -0.65766584  0.65095722\n",
      " -1.08491251 -0.94092401 -1.33684418 -3.08773976 -3.30981173 -2.4269322\n",
      " -0.61530556 -0.53807387  0.91715212  1.76406575 -1.39470631 -0.5269723\n",
      " -2.87728822 -2.32565724 -2.47496251 -3.03086345 -0.90794951  1.69986259\n",
      "  3.15063312  5.03236816  4.27523186  3.19537504  3.03571891  2.81420754\n",
      "  4.90390081  7.21127166  9.84788735],[3.25075899],x_train,y_train)=[3070285.61615603]\n",
      "cost([ 1.66901917 -0.72731333  2.19396808  3.33390524  1.21099675  3.10689162\n",
      "  4.80588031  9.05767084 11.98302643  0.08081823 -1.62744669 -1.29691109\n",
      " -0.98877803 -1.14215684  0.02239524  0.38142467  2.44981094  0.6760338\n",
      " -0.31720838 -0.23418485 -0.80314459  1.48277368 -0.3341162   0.85454015\n",
      " -0.92147721 -0.8529293  -1.32795879 -3.17877212 -3.42606092 -2.52908288\n",
      " -0.66194632 -0.57883949  0.96369176  1.94938574 -1.31362512 -0.47047249\n",
      " -2.8366252  -2.20880288 -2.49337386 -3.10163633 -0.9572016   1.64299269\n",
      "  3.22517331  5.34997631  4.49885332  3.20661781  2.82960471  2.51678943\n",
      "  4.62933471  7.07256566  9.91828997],[3.3982193],x_train,y_train)=[2391664.57141638]\n",
      "cost([ 1.39450176 -0.97460259  1.93298109  3.15794511  1.08102716  3.03639175\n",
      "  4.80174956  9.22077649 12.30844149  0.06515677 -1.52701285 -1.14441028\n",
      " -0.86125351 -0.8085595   0.05411291  0.20161149  2.43230719  0.46258148\n",
      " -0.44582972 -0.32296717 -0.60172599  1.67661674 -0.11106449  1.01370333\n",
      " -0.79727334 -0.76910053 -1.26302711 -3.16758181 -3.41541597 -2.57487507\n",
      " -0.68345847 -0.58527798  0.98756248  2.04075658 -1.32034792 -0.4577326\n",
      " -2.8384135  -2.15657469 -2.49261644 -3.09363261 -0.89351958  1.68683794\n",
      "  3.35193436  5.64230901  4.62713768  3.13316542  2.63133733  2.22120831\n",
      "  4.3804928   6.93677895  9.94828731],[3.52484178],x_train,y_train)=[1959812.41182021]\n",
      "cost([ 1.16171318 -1.19762151  1.71950008  3.0167596   1.01117234  3.016061\n",
      "  4.84679182  9.39597158 12.64036236  0.0677978  -1.40182766 -0.98053663\n",
      " -0.71094161 -0.54919646  0.05409886  0.01650767  2.40392914  0.2876837\n",
      " -0.52118795 -0.34500269 -0.43190559  1.8244723   0.03923397  1.12470432\n",
      " -0.71320221 -0.70052653 -1.20518767 -3.13970033 -3.38278349 -2.61440538\n",
      " -0.6922163  -0.58770981  1.00538543  2.09856567 -1.32378731 -0.4352337\n",
      " -2.80378988 -2.06601761 -2.45339259 -3.05385733 -0.82244569  1.74119443\n",
      "  3.4578518   5.85490766  4.65826389  3.00424712  2.38536434  1.88473353\n",
      "  4.10623315  6.77234691  9.92689132],[3.64710829],x_train,y_train)=[1642706.55386095]\n",
      "cost([ 0.97591169 -1.38644014  1.56311172  2.92663768  0.99893974  3.04536397\n",
      "  4.94215531  9.59272612 12.98994191  0.07075861 -1.27175239 -0.82045944\n",
      " -0.56184294 -0.3570856   0.03406027 -0.15544436  2.38480212  0.160889\n",
      " -0.54512579 -0.32176745 -0.29507101  1.92895345  0.12639817  1.19672643\n",
      " -0.65721677 -0.64341462 -1.15032269 -3.09124921 -3.32146763 -2.63083518\n",
      " -0.67772819 -0.57210389  1.0377003   2.15276233 -1.31558885 -0.40479001\n",
      " -2.75221306 -1.9685335  -2.4036618  -3.00857673 -0.76530809  1.7864069\n",
      "  3.5312104   5.98897441  4.60572188  2.81249013  2.08880395  1.50613576\n",
      "  3.80548561  6.57988422  9.86001685],[3.7663317],x_train,y_train)=[1385049.56127885]\n",
      "cost([ 8.30972792e-01 -1.54074244e+00  1.45618437e+00  2.87794216e+00\n",
      "  1.03024006e+00  3.11254184e+00  5.07205231e+00  9.79832235e+00\n",
      "  1.33398325e+01  7.18849738e-02 -1.14268788e+00 -6.70484770e-01\n",
      " -4.23985740e-01 -2.18533261e-01  1.91948090e-03 -3.09100839e-01\n",
      "  2.37544005e+00  7.50697275e-02 -5.31231391e-01 -2.73767419e-01\n",
      " -1.89283997e-01  1.99814236e+00  1.67429534e-01  1.24014633e+00\n",
      " -6.21395229e-01 -5.94043974e-01 -1.09568771e+00 -3.02666974e+00\n",
      " -3.23925544e+00 -2.63039123e+00 -6.47673662e-01 -5.44293057e-01\n",
      "  1.07715886e+00  2.19832987e+00 -1.30196885e+00 -3.71232093e-01\n",
      " -2.69237844e+00 -1.87211892e+00 -2.34588364e+00 -2.95788180e+00\n",
      " -7.16393978e-01  1.82471090e+00  3.57747536e+00  6.05852106e+00\n",
      "  4.48778594e+00  2.57766447e+00  1.76863460e+00  1.10942301e+00\n",
      "  3.49475730e+00  6.37157506e+00  9.75939145e+00],[3.87890012],x_train,y_train)=[1168880.66164568]\n",
      "cost([ 0.72042127 -1.66211857  1.39012603  2.86426965  1.09602325  3.20677441\n",
      "  5.22424952 10.0052368  13.68088505  0.07220194 -1.01781226 -0.53295963\n",
      " -0.30214934 -0.12258749 -0.03761819 -0.44331132  2.37536762  0.02232391\n",
      " -0.49169286 -0.21566862 -0.11144666  2.03967568  0.17660973  1.26248922\n",
      " -0.5998192  -0.55087458 -1.04343228 -2.95459417 -3.14787487 -2.6198982\n",
      " -0.60806863 -0.50956624  1.12072562  2.23747659 -1.28185127 -0.3350552\n",
      " -2.62715143 -1.77810045 -2.28224862 -2.90562223 -0.67785989  1.85273291\n",
      "  3.59784155  6.07390394  4.32143495  2.31703489  1.44201887  0.71072192\n",
      "  3.18447734  6.1563313   9.63574349],[3.98354527],x_train,y_train)=[984395.04497125]\n",
      "cost([ 6.38627208e-01 -1.75353964e+00  1.35706788e+00  2.87946254e+00\n",
      "  1.18702007e+00  3.31829478e+00  5.38820594e+00  1.02075591e+01\n",
      "  1.40063179e+01  7.14662988e-02 -9.00173469e-01 -4.09786248e-01\n",
      " -1.99109803e-01 -5.99069469e-02 -8.06888066e-02 -5.57321480e-01\n",
      "  2.38446802e+00 -4.20460960e-03 -4.36354415e-01 -1.57338275e-01\n",
      " -5.71796458e-02  2.06032790e+00  1.64819995e-01  1.26984625e+00\n",
      " -5.87612205e-01 -5.12759833e-01 -9.94736627e-01 -2.88076183e+00\n",
      " -3.05472403e+00 -2.60313706e+00 -5.62772919e-01 -4.70913160e-01\n",
      "  1.16713904e+00  2.27332308e+00 -1.25490148e+00 -2.97263652e-01\n",
      " -2.56041240e+00 -1.68967030e+00 -2.21618762e+00 -2.85570895e+00\n",
      " -6.51231265e-01  1.86826350e+00  3.59452398e+00  6.04582207e+00\n",
      "  4.12221135e+00  2.04398036e+00  1.12230745e+00  3.22782921e-01\n",
      "  2.88349677e+00  5.94218225e+00  9.49922231e+00],[4.07958754],x_train,y_train)=[826899.84770153]\n",
      "cost([ 5.80204349e-01 -1.81904235e+00  1.34930579e+00  2.91677269e+00\n",
      "  1.29426117e+00  3.43826817e+00  5.55454770e+00  1.03999941e+01\n",
      "  1.43102962e+01  6.96156814e-02 -7.91706674e-01 -3.01880356e-01\n",
      " -1.15532663e-01 -2.23713964e-02 -1.24428289e-01 -6.51464788e-01\n",
      "  2.40186796e+00 -1.10505446e-02 -3.73322142e-01 -1.04989933e-01\n",
      " -2.16876625e-02  2.06605040e+00  1.40521792e-01  1.26706315e+00\n",
      " -5.81108834e-01 -4.78847254e-01 -9.50284681e-01 -2.80930717e+00\n",
      " -2.96494695e+00 -2.58307571e+00 -5.14998008e-01 -4.30843065e-01\n",
      "  1.21483629e+00  2.30755833e+00 -1.22180740e+00 -2.59105825e-01\n",
      " -2.49550677e+00 -1.60923761e+00 -2.15022674e+00 -2.81057307e+00\n",
      " -6.36441794e-01  1.87083304e+00  3.57088600e+00  5.98494721e+00\n",
      "  3.90377994e+00  1.76993070e+00  8.20606155e-01 -4.38189234e-02\n",
      "  2.59950902e+00  5.73628158e+00  9.35895344e+00],[4.16660579],x_train,y_train)=[694375.81950719]\n",
      "cost([ 5.40246798e-01 -1.86306222e+00  1.35982296e+00  2.96983765e+00\n",
      "  1.40998015e+00  3.55926362e+00  5.71565080e+00  1.05784990e+01\n",
      "  1.45886977e+01  6.68048547e-02 -6.93418419e-01 -2.09280776e-01\n",
      " -5.05925739e-02 -3.22357104e-03 -1.66893961e-01 -7.26902248e-01\n",
      "  2.42632707e+00 -4.02689136e-03 -3.08812907e-01 -6.19397726e-02\n",
      " -3.54947671e-04  2.06177709e+00  1.09993344e-01  1.25778542e+00\n",
      " -5.77705512e-01 -4.48581335e-01 -9.10590565e-01 -2.74322789e+00\n",
      " -2.88205935e+00 -2.56195543e+00 -4.67204144e-01 -3.91327924e-01\n",
      "  1.26243360e+00  2.34127760e+00 -1.18346938e+00 -2.21660849e-01\n",
      " -2.43490879e+00 -1.53820195e+00 -2.08620811e+00 -2.77180951e+00\n",
      " -6.32889078e-01  1.86075044e+00  3.53051969e+00  5.90100382e+00\n",
      "  3.67756474e+00  1.50403957e+00  5.45051868e-01 -3.81328931e-01\n",
      "  2.33834467e+00  5.54432189e+00  9.22254234e+00],[4.24453813],x_train,y_train)=[585203.8141353]\n",
      "cost([ 5.14472120e-01 -1.88999362e+00  1.38258739e+00  3.03302211e+00\n",
      "  1.52775713e+00  3.67540566e+00  5.86571362e+00  1.07403091e+01\n",
      "  1.48390420e+01  6.32308927e-02 -6.05663191e-01 -1.31435710e-01\n",
      " -2.57387221e-03  3.01057629e-03 -2.06842760e-01 -7.85259015e-01\n",
      "  2.45650466e+00  1.19952073e-02 -2.47238048e-01 -2.93656523e-02\n",
      "  1.09431113e-02  2.05139763e+00  7.76398103e-02  1.24465813e+00\n",
      " -5.75643978e-01 -4.21596283e-01 -8.75918318e-01 -2.68442914e+00\n",
      " -2.80809450e+00 -2.54128715e+00 -4.21128959e-01 -3.53769654e-01\n",
      "  1.30881216e+00  2.37513469e+00 -1.14093766e+00 -1.85851224e-01\n",
      " -2.38041013e+00 -1.47728883e+00 -2.02548178e+00 -2.74035796e+00\n",
      " -6.39591941e-01  1.83888249e+00  3.47699189e+00  5.80250496e+00\n",
      "  3.45270323e+00  1.25315305e+00  3.01012679e-01 -6.84661571e-01\n",
      "  2.10398181e+00  5.37045749e+00  9.09592698e+00],[4.31360645],x_train,y_train)=[497324.98350338]\n",
      "cost([ 4.99237644e-01 -1.90391319e+00  1.41262388e+00  3.10154115e+00\n",
      "  1.64254177e+00  3.78234053e+00  6.00064168e+00  1.08838054e+01\n",
      "  1.50602559e+01  5.91087399e-02 -5.28302688e-01 -6.73819870e-02\n",
      "  3.07307919e-02  6.63601536e-04 -2.43567802e-01 -8.28417372e-01\n",
      "  2.49106391e+00  3.30993382e-02 -1.91453981e-01 -6.98743990e-03\n",
      "  1.56677951e-02  2.03783649e+00  4.63717757e-02  1.22952557e+00\n",
      " -5.73826412e-01 -3.97640820e-01 -8.46296727e-01 -2.63392003e+00\n",
      " -2.74393698e+00 -2.52202447e+00 -3.77933387e-01 -3.19111484e-01\n",
      "  1.35306840e+00  2.40938372e+00 -1.09534579e+00 -1.52430793e-01\n",
      " -2.33318948e+00 -1.42665985e+00 -1.96894763e+00 -2.71659237e+00\n",
      " -6.55291203e-01  1.80649109e+00  3.41367991e+00  5.69663377e+00\n",
      "  3.23615811e+00  1.02204875e+00  9.14831044e-02 -9.51049693e-01\n",
      "  1.89871049e+00  5.21737688e+00  8.98339409e+00],[4.3742363],x_train,y_train)=[428123.88361777]\n",
      "cost([ 4.91525123e-01 -1.90840683e+00  1.44600428e+00  3.17152330e+00\n",
      "  1.75060522e+00  3.87712061e+00  6.11787648e+00  1.10083719e+01\n",
      "  1.52524534e+01  5.46534308e-02 -4.60848981e-01 -1.58905576e-02\n",
      "  5.17245133e-02 -6.92785985e-03 -2.76762793e-01 -8.58359048e-01\n",
      "  2.52875321e+00  5.62755438e-02 -1.43044562e-01  6.37885484e-03\n",
      "  1.65562242e-02  2.02317337e+00  1.79502262e-02  1.21360973e+00\n",
      " -5.71647457e-01 -3.76528114e-01 -8.21565487e-01 -2.59202170e+00\n",
      " -2.68963955e+00 -2.50470636e+00 -3.38320927e-01 -2.87927158e-01\n",
      "  1.39450662e+00  2.44399137e+00 -1.04781593e+00 -1.21967218e-01\n",
      " -2.29388727e+00 -1.38602415e+00 -1.91712902e+00 -2.70045194e+00\n",
      " -6.78589884e-01  1.76505610e+00  3.34363546e+00  5.58920990e+00\n",
      "  3.03292045e+00  8.13704233e-01 -8.25325372e-02 -1.17968116e+00\n",
      "  1.72333568e+00  5.08644008e+00  8.88769655e+00],[4.42699727],x_train,y_train)=[374606.84767821]\n",
      "cost([ 4.88900097e-01 -1.90648542e+00  1.47977537e+00  3.23999487e+00\n",
      "  1.84941386e+00  3.95802941e+00  6.21617339e+00  1.11142211e+01\n",
      "  1.54166944e+01  5.00610710e-02 -4.02588529e-01  2.44113649e-02\n",
      "  6.27935243e-02 -1.72720554e-02 -3.06405810e-01 -8.77045074e-01\n",
      "  2.56845895e+00  7.93289384e-02 -1.02607962e-01  1.23888879e-02\n",
      "  1.56429660e-02  2.00878529e+00 -6.71212818e-03  1.19766875e+00\n",
      " -5.68848370e-01 -3.58099116e-01 -8.01416878e-01 -2.55854643e+00\n",
      " -2.64468303e+00 -2.48957057e+00 -3.02641948e-01 -2.60496980e-01\n",
      "  1.43262881e+00  2.47873763e+00 -9.99395415e-01 -9.48451572e-02\n",
      " -2.26270075e+00 -1.35476163e+00 -1.87025229e+00 -2.69155930e+00\n",
      " -7.08057622e-01  1.71614222e+00  3.26951166e+00  5.48474640e+00\n",
      "  2.84626994e+00  6.29587131e-01 -2.21619235e-01 -1.37131001e+00\n",
      "  1.57742832e+00  4.97787264e+00  8.81024257e+00],[4.47255045],x_train,y_train)=[333684.94562278]\n",
      "cost([ 4.89452374e-01 -1.90056801e+00  1.51184426e+00  3.30480793e+00\n",
      "  1.93746105e+00  4.02437484e+00  6.29535664e+00  1.12022087e+01\n",
      "  1.55547425e+01  4.54984944e-02 -3.52679514e-01  5.49462772e-02\n",
      "  6.61675811e-02 -2.85747558e-02 -3.32665555e-01 -8.86334033e-01\n",
      "  2.60923044e+00  1.00761175e-01 -7.00267165e-02  1.28509452e-02\n",
      "  1.43328911e-02  1.99549310e+00 -2.72970480e-02  1.18213015e+00\n",
      " -5.65398278e-01 -3.42198710e-01 -7.85437051e-01 -2.53295387e+00\n",
      " -2.60819089e+00 -2.47664921e+00 -2.70986409e-01 -2.36877870e-01\n",
      "  1.46711616e+00  2.51329648e+00 -9.51016118e-01 -7.12803203e-02\n",
      " -2.23947771e+00 -1.33203091e+00 -1.82831661e+00 -2.68931966e+00\n",
      " -7.42305309e-01  1.66130274e+00  3.19353653e+00  5.38656529e+00\n",
      "  2.67805868e+00  4.69952050e-01 -3.27561301e-01 -1.52787315e+00\n",
      "  1.45959317e+00  4.89098623e+00  8.75132254e+00],[4.51160412],x_train,y_train)=[302438.30024378]\n",
      "cost([ 4.91727614e-01 -1.89251156e+00  1.54084381e+00  3.36453697e+00\n",
      "  2.01408276e+00  4.07627517e+00  6.35607585e+00  1.12736551e+01\n",
      "  1.56688414e+01  4.10988692e-02 -3.10225744e-01  7.71169318e-02\n",
      "  6.38368831e-02 -3.96032254e-02 -3.55828278e-01 -8.87930665e-01\n",
      "  2.65028632e+00  1.19643339e-01 -4.47041574e-02  9.50676190e-03\n",
      "  1.35015508e-02  1.98369932e+00 -4.38771362e-02  1.16719744e+00\n",
      " -5.61401964e-01 -3.28662452e-01 -7.73144922e-01 -2.51448145e+00\n",
      " -2.57909816e+00 -2.46584409e+00 -2.43261151e-01 -2.16963702e-01\n",
      "  1.49780763e+00  2.54730030e+00 -9.03470198e-01 -5.13395892e-02\n",
      " -2.22380343e+00 -1.31685998e+00 -1.79115511e+00 -2.69300373e+00\n",
      " -7.80038139e-01  1.60201118e+00  3.11751798e+00  5.29694559e+00\n",
      "  2.52899355e+00  3.34122252e-01 -4.02988223e-01 -1.65213863e+00\n",
      "  1.36772854e+00  4.82440133e+00  8.71034653e+00],[4.54487864],x_train,y_train)=[278303.43952814]\n",
      "cost([ 4.94656690e-01 -1.88366995e+00  1.56599361e+00  3.41836042e+00\n",
      "  2.07927397e+00  4.11445401e+00  6.39957935e+00  1.13301843e+01\n",
      "  1.57615197e+01  3.69611249e-02 -2.74330501e-01  9.22572991e-02\n",
      "  5.75112509e-02 -4.95651140e-02 -3.76242525e-01 -8.83357672e-01\n",
      "  2.69100844e+00  1.35492268e-01 -2.57598007e-02  3.89514807e-03\n",
      "  1.36055255e-02  1.97350974e+00 -5.67596470e-02  1.15293188e+00\n",
      " -5.57031706e-01 -3.17311101e-01 -7.64026040e-01 -2.50224787e+00\n",
      " -2.55627686e+00 -2.45698301e+00 -2.19251765e-01 -2.00535258e-01\n",
      "  1.52467710e+00  2.58038790e+00 -8.57400059e-01 -3.49648662e-02\n",
      " -2.21507856e+00 -1.30822009e+00 -1.75848667e+00 -2.70181433e+00\n",
      " -8.20089617e-01  1.53961696e+00  3.04287067e+00  5.21728615e+00\n",
      "  2.39890055e+00  2.20743942e-01 -4.51067086e-01 -1.74739828e+00\n",
      "  1.29926372e+00  4.77625684e+00  8.68607378e+00],[4.57307966],x_train,y_train)=[259171.79807123]\n",
      "cost([ 4.97487322e-01 -1.87496864e+00  1.58696705e+00  3.46593949e+00\n",
      "  2.13351733e+00  4.14005496e+00  6.42751342e+00  1.13735855e+01\n",
      "  1.58354276e+01  3.31521182e-02 -2.44133559e-01  1.01600331e-01\n",
      "  4.86098424e-02 -5.80038837e-02 -3.94278942e-01 -8.73944957e-01\n",
      "  2.73092784e+00  1.48157687e-01 -1.21822582e-02 -2.71932793e-03\n",
      "  1.47902747e-02  1.96483505e+00 -6.63721249e-02  1.13931238e+00\n",
      " -5.52479833e-01 -3.07950557e-01 -7.57560540e-01 -2.49533167e+00\n",
      " -2.53862385e+00 -2.44985984e+00 -1.98670488e-01 -1.87300529e-01\n",
      "  1.54780982e+00  2.61223714e+00 -8.13299328e-01 -2.19982744e-02\n",
      " -2.21258592e+00 -1.30508282e+00 -1.72995810e+00 -2.71493772e+00\n",
      " -8.61439972e-01  1.47532006e+00  2.97065651e+00  5.14826927e+00\n",
      "  2.28696106e+00  1.28006752e-01 -4.75246257e-01 -1.81721060e+00\n",
      "  1.25136527e+00  4.74439642e+00  8.67682140e+00],[4.59687884],x_train,y_train)=[243411.50107403]\n",
      "cost([ 4.99720973e-01 -1.86698467e+00  1.60377148e+00  3.50730314e+00\n",
      "  2.17763161e+00  4.15448212e+00  6.44175260e+00  1.14056993e+01\n",
      "  1.58932062e+01  2.97105047e-02 -2.18834728e-01  1.06259078e-01\n",
      "  3.82717082e-02 -6.47110020e-02 -4.10302388e-01 -8.60831355e-01\n",
      "  2.76970630e+00  1.57724389e-01 -2.94201199e-03 -9.36934682e-03\n",
      "  1.69874714e-02  1.95747170e+00 -7.31831278e-02  1.12627718e+00\n",
      " -5.47928166e-01 -3.00375253e-01 -7.53244998e-01 -2.49282780e+00\n",
      " -2.52511773e+00 -2.44426134e+00 -1.81191873e-01 -1.76926236e-01\n",
      "  1.56737931e+00  2.64258462e+00 -7.71521708e-01 -1.22068459e-02\n",
      " -2.21554579e+00 -1.30646198e+00 -1.70517736e+00 -2.73158132e+00\n",
      " -9.03222566e-01  1.41016031e+00  2.90163192e+00  5.09001547e+00\n",
      "  2.19191431e+00  5.38278074e-02 -4.79050691e-01 -1.86519506e+00\n",
      "  1.22110804e+00  4.72652611e+00  8.68064497e+00],[4.61690078],x_train,y_train)=[229834.23290736]\n",
      "cost([ 5.01056772e-01 -1.86002453e+00  1.61664503e+00  3.54274471e+00\n",
      "  2.21264373e+00  4.15926889e+00  6.44426241e+00  1.14283293e+01\n",
      "  1.59373896e+01  2.66514666e-02 -1.97707034e-01  1.07218044e-01\n",
      "  2.73794825e-02 -6.96541073e-02 -4.24653797e-01 -8.44974753e-01\n",
      "  2.80711624e+00  1.64431193e-01  2.93124845e-03 -1.53743571e-02\n",
      "  1.99973865e-02  1.95116290e+00 -7.76510121e-02  1.11375095e+00\n",
      " -5.43530559e-01 -2.94373377e-01 -7.50608444e-01 -2.49388587e+00\n",
      " -2.51485108e+00 -2.43998370e+00 -1.66478267e-01 -1.69061728e-01\n",
      "  1.58362569e+00  2.67123485e+00 -7.32295026e-01 -5.30542860e-03\n",
      " -2.22315972e+00 -1.31144225e+00 -1.68373876e+00 -2.75100000e+00\n",
      " -9.44721454e-01  1.34501724e+00  2.83629704e+00  5.04222317e+00\n",
      "  2.11222503e+00 -4.00024381e-03 -4.65925849e-01 -1.89487542e+00\n",
      "  1.20561014e+00  4.72034132e+00  8.69548753e+00],[4.63371518],x_train,y_train)=[217629.9268522]\n",
      "cost([ 5.01343214e-01 -1.85419507e+00  1.62597169e+00  3.57273332e+00\n",
      "  2.23968506e+00  4.15597499e+00  6.43699332e+00  1.14431780e+01\n",
      "  1.59703353e+01  2.39716750e-02 -1.80102264e-01  1.05332129e-01\n",
      "  1.65903294e-02 -7.29197221e-02 -4.37639490e-01 -8.27167395e-01\n",
      "  2.84302057e+00  1.68606596e-01  6.30084616e-03 -2.03089978e-02\n",
      "  2.35546031e-02  1.94564196e+00 -8.01941257e-02  1.10166094e+00\n",
      " -5.39405087e-01 -2.89732748e-01 -7.49223113e-01 -2.49773344e+00\n",
      " -2.50704450e+00 -2.43684136e+00 -1.54197125e-01 -1.63356466e-01\n",
      "  1.59683588e+00  2.69806161e+00 -6.95738240e-01 -9.76995559e-04\n",
      " -2.23464415e+00 -1.31919688e+00 -1.66524140e+00 -2.77251285e+00\n",
      " -9.85362780e-01  1.28061729e+00  2.77494340e+00  5.00429003e+00\n",
      "  2.04621697e+00 -4.76948278e-02 -4.39125108e-01 -1.90956711e+00\n",
      "  1.20213379e+00  4.72362377e+00  8.71929696e+00],[4.64783318],x_train,y_train)=[206288.31009352]\n",
      "cost([ 5.00537738e-01 -1.84946487e+00  1.63221408e+00  3.59784115e+00\n",
      "  2.25991131e+00  4.14610966e+00  6.42180325e+00  1.14518029e+01\n",
      "  1.59941801e+01  2.16540555e-02 -1.65451126e-01  1.01330956e-01\n",
      "  6.36982979e-03 -7.46687533e-02 -4.49526028e-01 -8.08053925e-01\n",
      "  2.87735397e+00  1.70619922e-01  7.89870360e-03 -2.39602469e-02\n",
      "  2.73772626e-02  1.94066088e+00 -8.11765198e-02  1.08994511e+00\n",
      " -5.35632882e-01 -2.86246484e-01 -7.48710703e-01 -2.50368793e+00\n",
      " -2.50104756e+00 -2.43467055e+00 -1.44032037e-01 -1.59472296e-01\n",
      "  1.60732607e+00  2.72300367e+00 -6.61879644e-01  1.11007413e-03\n",
      " -2.24925466e+00 -1.32899660e+00 -1.64930194e+00 -2.79551255e+00\n",
      " -1.02470225e+00  1.21754562e+00  2.71769766e+00  4.97541471e+00\n",
      "  1.99217557e+00 -7.93989873e-02 -4.01634483e-01 -1.91230258e+00\n",
      "  1.20815547e+00  4.73431093e+00  8.75011339e+00],[4.6597067],x_train,y_train)=[195520.9440202]\n",
      "cost([ 4.98673771e-01 -1.84571470e+00  1.63586279e+00  3.61868623e+00\n",
      "  2.27444362e+00  4.13107814e+00  6.40040491e+00  1.14555904e+01\n",
      "  1.60108160e+01  1.96720776e-02 -1.53259854e-01  9.58267933e-02\n",
      " -2.97420892e-03 -7.51028964e-02 -4.60538983e-01 -7.88150366e-01\n",
      "  2.91010598e+00  1.70846046e-01  8.31839265e-03 -2.62806535e-02\n",
      "  3.12013051e-02  1.93600692e+00 -8.09042456e-02  1.07855503e+00\n",
      " -5.32261182e-01 -2.83717947e-01 -7.48744973e-01 -2.51116021e+00\n",
      " -2.49633129e+00 -2.43332933e+00 -1.35689093e-01 -1.57091600e-01\n",
      "  1.61542684e+00  2.74605693e+00 -6.30674926e-01  1.28788097e-03\n",
      " -2.26630258e+00 -1.34021202e+00 -1.63556290e+00 -2.81946897e+00\n",
      " -1.06241043e+00  1.15626053e+00  2.66456033e+00  4.95467939e+00\n",
      "  1.94842328e+00 -1.01122585e-01 -3.56128136e-01 -1.90578818e+00\n",
      "  1.22140968e+00  4.75054124e+00  8.78613007e+00],[4.66973018],x_train,y_train)=[185192.0896035]\n",
      "Optimization required, your accuracy is 55.5%\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype(np.float64)\n",
    "#x_train,x_std,x_mean = z_score(x_train)\n",
    "\n",
    "np.random.seed(2147483647)\n",
    "w = np.random.randn(x_train.shape[1],)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "old_cost = 0\n",
    "i=0\n",
    "while abs(old_cost - cost(x_train,y_train,w,b))>1:\n",
    "    old_cost = cost(x_train,y_train,w,b)\n",
    "    w,b = gradient_descent(x_train,y_train,w,b)\n",
    "    if(i%100==0):\n",
    "        print(f\"cost({w},{b},x_train,y_train)={cost(x_train,y_train,w,b)}\")\n",
    "    i+=1\n",
    "'''w=np.array([-0.79384013,-0.03146606,-3.47636683, 5.8895991, -1.5585789, 16.32260731,\n",
    " 22.70597599, 0.70833468, 1.76190363,-1.95699588, 4.92120563, 2.27833866,\n",
    "  1.77665849, 0.53578001,-2.30993616, -0.70841404, -6.84501382, 0.80395445,\n",
    " 16.73966405])\n",
    "b=np.array([32.56130664])'''\n",
    "x_predict = pd.read_excel('Test data.xlsx').iloc[:,:8].to_numpy()\n",
    "x_predict = feature_changing(x_predict)\n",
    "x_predict = (x_predict - x_mean)/x_std\n",
    "ans = pd.read_excel('Test data.xlsx').iloc[:,8].to_numpy()\n",
    "\n",
    "y_predict = np.dot(x_predict,w) + b\n",
    "\n",
    "accuracy = 0\n",
    "for dim in range(len(ans)):\n",
    "  if abs(y_predict[dim]-ans[dim])<0.5: # do not change the tolerance as you'll be checked on +- 0.5 error only\n",
    "    accuracy += 1\n",
    "accuracy = round(accuracy*100/200.0,2)\n",
    "ok = 'Congratulations' if accuracy>95 else 'Optimization required'\n",
    "print(f\"{ok}, your accuracy is {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Got these values after running on PC for 4 hrs(no loop breaks), 95% accuracy\n",
    "'''w2=np.array([3.13339739e-02,-1.15901188e+00,1.61904955e+00,4.06748069e+00\n",
    ",2.37758713e+00,3.24832874e+00,5.41648895e+00,1.12125540e+01\n",
    ",1.61480752e+01,2.50190651e-03,-1.91263984e-02,1.94336898e-03\n",
    ",-2.46528043e-02,-2.09377469e-02,-6.78711604e-01,-1.20306749e-02\n",
    ",4.06652123e+00,5.87050247e-03,-5.43958443e-03,6.46287090e-03\n",
    ",7.03175363e-03,1.21123775e+00,4.82691878e-03,1.65733088e-01\n",
    ",-2.95040730e-01,1.26000049e-01,-1.26207348e-01,-2.64905765e+00\n",
    ",-2.06078799e+00,-2.22135401e+00,9.27600918e-02,8.41489456e-02\n",
    ",1.89781457e+00,2.31975374e+00,-6.48168427e-01,-2.65808824e-01\n",
    ",-2.93285169e+00,-1.40596057e+00,-1.50971229e+00,-3.59682910e+00\n",
    ",-1.30548156e+00,3.88195635e-01,2.19212401e+00,5.25308050e+00\n",
    ",1.16753665e+00,2.14503105e-01,8.89913064e-01,-1.55736594e+00\n",
    ",1.22241342e+00,4.88347366e+00,9.53141215e+00])\n",
    "b2=np.array([4.96641022])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''x_predict = pd.read_excel('Test data.xlsx').iloc[:,:8].to_numpy()\n",
    "x_predict = feature_changing(x_predict)\n",
    "x_predict = (x_predict - x_mean)/x_std\n",
    "ans = pd.read_excel('Test data.xlsx').iloc[:,8].to_numpy()\n",
    "\n",
    "y_predict = np.dot(x_predict,w2) + b2\n",
    "\n",
    "accuracy = 0\n",
    "for dim in range(len(ans)):\n",
    "  if abs(y_predict[dim]-ans[dim])<0.5: # do not change the tolerance as you'll be checked on +- 0.5 error only\n",
    "    accuracy += 1\n",
    "accuracy = round(accuracy*100/200.0,2)\n",
    "ok = 'Congratulations' if accuracy>95 else 'Optimization required'\n",
    "print(f\"{ok}, your accuracy is {accuracy}%\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
